---
title: '[译文]：数据挖掘之独孤九剑（上）'
date: 2011-05-04 22:41
tags: [数据挖掘,业务,九定律,套路]
categories: 数据挖掘
---
【从网易博客迁移，本文写于2012年5月,有不少网站修改标题之后，上下两篇合并转载的，有的没注明来源，更没标注原作者，我这里只是生硬的翻译，很多词不达意的地方】

#### by [Tom Khabaza](http://khabaza.codimension.net/index.htm) 

鄙人是从一个 [大牛的博客](http://spss-market.r.blog.163.com/) 发现这篇文章的， [博主用简洁的语言对这篇文章进行了提炼](http://spss-market.r.blog.163.com/blog/static/731422682011116105231563/) ，小可用原文练习一下英语（在下英语实在很差），许多地方的翻译参考了博主的这篇文章，特此感谢！

原文在此： [1](http://khabaza.codimension.net/index_files/9laws.htm) , [2](http://khabaza.codimension.net/index_files/Page346.htm) , [3](http://khabaza.codimension.net/index_files/Page347.htm) 。
<!--more-->
**\[译文\]** **（上半部分）** **：**

本文在2010年第一季度出版“数据挖掘之九定律”时创作，它解释了数据挖掘过程中隐藏的原因。如果您喜欢简洁，请看我的微博：twitter.com/tomkhabaza。如果您是 [LinkedIn](http://www.linkedin.com/) 的一员，请看 [CRISP-DM](http://www.crisp-dm.org/) 讨论组的一个子组“数据挖掘之九定律”的讨论论坛。这九条定律以 [俳句的形式在这里给出](http://khabaza.codimension.net/index_files/Page353.htm) 。

数据挖掘是利用业务知识从数据中发现和解释知识（或称为模式）的过程，这种知识是以自然或者人工形式创造的新知识。

当前的数据挖掘形式，是在20世纪90年代实践领域诞生的，是在集成数据挖掘算法平台发展的支撑下适合商业分析的一种形式。也许是因为数据挖掘源于实践而非理论，在其过程的理解上不太引人注意。20世纪90年代晚期发展的CRISP-DM，逐渐成为数据挖掘过程的一种标准化过程，被越来越多的数据挖掘实践者成功运用和遵循。

虽然CRISP-DM能够指导如何实施数据挖掘，但是它不能解释数据挖掘是什么或者为什么适合这样做。在本文中我将阐述我提出数据挖掘的九种准则或“定律”（其中大多数为实践者所熟知）以及另外其它一些熟知的解释。开始从理论上（不仅仅是描述上）来解释数据挖掘过程。

我的目的不是评论CRISP-DM，但CRISP-DM的许多概念对于理解数据挖掘是至关重要的，本文也将依赖于CRISP-DM的常见术语。CRISP-DM仅仅是论述这个过程的开始。

**第一，目标律：业务目标是所有数据解决方案的源头。**

它定义了数据挖掘的主题：数据挖掘关注解决业务业问题和实现业务目标。数据挖掘主要不是一种技术，而是一个过程，业务目标是它的的核心。 没有业务目标，没有数据挖掘（不管这种表述是否清楚）。因此这个准则也可以说成：数据挖掘是业务过程。

** 第二，知识律：业务知识是数据挖掘过程每一步的核心。**

这里定义了数据挖掘过程的一个关键特征。CRISP-DM的一种朴素的解读是业务知识仅仅作用于数据挖掘过程开始的目标的定义与最后的结果的实施，这将错过数据挖掘过程的一个关键属性，即业务知识是每一步的核心。

为了方便理解，我使用CRISP-DM阶段来说明：

*   商业理解必须基于业务知识，所以数据挖掘目标必须是业务目标的映射（这种映射也基于数据知识和数据挖掘知识）；
*   数据理解使用业务知识理解与业务问题相关的数据，以及它们是如何相关的；
*   数据预处理就是利用业务知识来塑造数据，使得业务问题可以被提出和解答（更详尽的第三条—准备律）；
*   建模是使用数据挖掘算法创建预测模型，同时解释模型和业务目标的特点，也就是说理解它们之间的业务相关性；
*   评估是模型对理解业务的影响；
*   实施是将数据挖掘结果作用于业务过程；

总之，没有业务知识，数据挖掘过程的每一步都是无效的，也没有“纯粹的技术”步骤。 业务知识指导过程产生有益的结果，并使得那些有益的结果得到认可。数据挖掘是一个反复的过程，业务知识是它的核心，驱动着结果的持续改善。

这背后的原因可以用“鸿沟的表现”（chasm of representation）来解释（Alan Montgomery在20世纪90年代对数据挖掘提出的一个观点）。Montgomery指出数据挖掘目标涉及到现实的业务，然而数据仅能表示现实的一部分；数据和现实世界是有差距（或“鸿沟”）的。在数据挖掘过程中，业务知识来弥补这一差距，在数据中无论发现什么，只有使用业务知识解释才能显示其重要性，数据中的任何遗漏必须通过业务知识弥补。只有业务知识才能弥补这种缺失，这是业务知识为什么是数据挖掘过程每一步骤的核心的原因。

![CRISP-DM Diagram](译文-：数据挖掘之独孤九剑（上）/1914874266564392511.bmp "CRISP-DM Diagram")


** 第三，准备律：数据预处理比数据挖掘其他任何一个过程都重要。**

这是数据挖掘著名的格言，数据挖掘项目中最费力的事是数据获取和预处理。非正式估计，其占用项目的时间为50%-80%。最简单的解释可以概括为“数据是困难的”，经常采用自动化减轻这个“问题”的数据获取、数据清理、数据转换等数据预处理各部分的工作量。虽然自动化技术是有益的，支持者相信这项技术可以减少数据预处理过程中的大量的工作量，但这也是误解数据预处理在数据挖掘过程中是必须的原因。

数据预处理的目的是把数据挖掘问题转化为格式化的数据，使得分析技术（如数据挖掘算法）更容易利用它。数据任何形式的变化（包括清理、最大最小值转换、增长等）意味着问题空间的变化，因此这种分析必须是探索性的。 这是数据预处理重要的原因，并且在数据挖掘过程中占有如此大的工作量，这样数据挖掘者可以从容地操纵问题空间，使得容易找到适合分析他们的方法。

有两种方法“塑造”这个问题空间。第一种方法是将数据转化为可以分析的完全格式化的数据，比如，大多数数据挖掘算法需要单一表格形式的数据，一个记录就是一个样例。数据挖掘者都知道什么样的算法需要什么样的数据形式，因此可以将数据转化为一个合适的格式。第二种方法是使得数据能够含有业务问题的更多的信息，例如，某些领域的一些数据挖掘问题，数据挖掘者可以通过业务知识和数据知识知道这些。 通过这些领域的知识，数据挖掘者通过操纵问题空间可能更容易找到一个合适的技术解决方案。

因此，通过业务知识、数据知识、数据挖掘知识从根本上使得数据预处理更加得心应手。 数据预处理的这些方面并不能通过简单的自动化实现。

这个定律也解释了一个有疑义的现象，也就是虽然经过数据获取、清理、融合等方式创建一个数据仓库，但是数据预处理仍然是必不可少的，仍然占有数据挖掘过程一半以上的工作量。此外，就像CRISP-DM展示的那样，即使经过了主要的数据预处理阶段，在创建一个有用的模型的反复过程中，进一步的数据预处理的必要的。

**第四，试验律（NFL律：No Free Lunch）：对于数据挖掘者来说，天下没有免费的午餐，一个正确的模型只有通过试验（experiment）才能被发现。**

机器学习有一个原则：如果我们充分了解一个问题空间（problem space），我们可以选择或设计一个找到最优方案的最有效的算法。一个卓越算法的参数依赖于数据挖掘问题空间一组特定的属性集，这些属性可以通过分析发现或者算法创建。但是，这种观点来自于一个错误的思想，在数据挖掘过程中数据挖掘者将问题公式化，然后利用算法找到解决方法。事实上，数据挖掘者将问题公式化和寻找解决方法是同时进行的-----算法仅仅是帮助数据挖掘者的一个工具。

有五种因素说明试验对于寻找数据挖掘解决方案是必要的： 

*   数据挖掘项目的业务目标定义了兴趣范围（定义域），数据挖掘目标反映了这一点；
*   与业务目标相关的数据及其相应的数据挖掘目标是在这个定义域上的数据挖掘过程产生的；
*   这些过程受规则限制，而这些过程产生的数据反映了这些规则；
*   在这些过程中，数据挖掘的目的是通过模式发现技术（数据挖掘算法）和可以解释这个算法结果的业务知识相结合的方法来揭示这个定义域上的规则；
*   数据挖掘需要在这个域上生成相关数据，这些数据含有的模式不可避免地受到这些规则的限制。

在这里强调一下最后一点，在数据挖掘中改变业务目标，CRISP-DM有所暗示，但经常不易被觉察到。广为所知的CRISP-DM过程不是下一个步骤仅接着上一个步骤的“瀑布”式的过程。事实上，在项目中的任何地方都可以进行任何CRISP-DM步骤，同样商业理解也可以存在于任何一个步骤。业务目标不是简单地在开始就给定，它贯穿于整个过程。这也许可以解释一些数据挖掘者在没有清晰的业务目标的情况下开始项目，他们知道业务目标也是数据挖掘的一个结果，不是静态地给定。

Wolpert的“没有免费的午餐”理论已经应用于机器学习领域，无偏的状态好于（如一个具体的算法）任何其他可能的问题（数据集）出现的平均状态。这是因为，如果我们考虑所有可能的问题，他们的解决方法是均匀分布的，以至于一个算法（或偏倚）对一个子集是有利的，而对另一个子集是不利的。这与数据挖掘者所知的具有惊人的相似性，没有一个算法适合每一个问题。但是经过数据挖掘处理的问题或数据集绝不是随机的，也不是所有可能问题的均匀分布，他们代表的是一个有偏差的样本，那么为什么要应用NFL的结论？答案涉及到上面提到的因素：问题空间初始是未知的，多重问题空间可能和每一个数据挖掘目标相关，问题空间可能被数据预处理所操纵，模型不能通过技术手段评估，业务问题本身可能会变化。由于这些原因，数据挖掘问题空间在数据挖掘过程中展开，并且在这个过程中是不断变化的，以至于在有条件的约束下，用算法模拟一个随机选择的数据集是有效的。 对于数据挖掘者来说：没有免费的午餐。

这大体上描述了数据挖掘过程。但是，在有条件限制某些情况下，比如业务目标是稳定的，数据和其预处理是稳定的，一个可接受的算法或算法组合可以解决这个问题。在这些情况下，一般的数据挖掘过程中的步骤将会减少。 但是，如果这种情况稳定是持续的，数据挖掘者的午餐是免费的，或者至少相对便宜的。像这样的稳定性是临时的，因为对数据的业务理解（第二律）和对问题的理解（第九律）都会变化的 _。（感谢Chris Thornton of Sussex大学对这个定律提出的帮助）_

* [网易博客原文Google快照](http://webcache.googleusercontent.com/search?q=cache:tsNaCLjbSacJ:qxde01.blog.163.com/blog/static/67335744201144103147739/+&cd=1&hl=zh-CN&ct=clnk)
